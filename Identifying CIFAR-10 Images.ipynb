{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 3s 0us/step\n",
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Augmenting training set images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:52: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,676,842\n",
      "Trainable params: 1,676,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 109s 3ms/step - loss: 1.7879 - accuracy: 0.3550 - val_loss: 1.3975 - val_accuracy: 0.4902\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 111s 3ms/step - loss: 1.3419 - accuracy: 0.5209 - val_loss: 1.2190 - val_accuracy: 0.5612\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 111s 3ms/step - loss: 1.1195 - accuracy: 0.6098 - val_loss: 0.9593 - val_accuracy: 0.6624\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 113s 3ms/step - loss: 0.9825 - accuracy: 0.6553 - val_loss: 1.0357 - val_accuracy: 0.6347\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 113s 3ms/step - loss: 0.8819 - accuracy: 0.6934 - val_loss: 0.8039 - val_accuracy: 0.7237\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 116s 3ms/step - loss: 0.8089 - accuracy: 0.7165 - val_loss: 0.8227 - val_accuracy: 0.7129\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 113s 3ms/step - loss: 0.7509 - accuracy: 0.7377 - val_loss: 0.8878 - val_accuracy: 0.7082\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 155s 4ms/step - loss: 0.7121 - accuracy: 0.7540 - val_loss: 0.7835 - val_accuracy: 0.7437\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 109s 3ms/step - loss: 0.6739 - accuracy: 0.7681 - val_loss: 0.7874 - val_accuracy: 0.7246\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 106s 3ms/step - loss: 0.6393 - accuracy: 0.7783 - val_loss: 0.7597 - val_accuracy: 0.7422\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 154s 4ms/step - loss: 0.6097 - accuracy: 0.7911 - val_loss: 0.7065 - val_accuracy: 0.7634\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.5945 - accuracy: 0.7958 - val_loss: 0.7609 - val_accuracy: 0.7666\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.5785 - accuracy: 0.8039 - val_loss: 0.7968 - val_accuracy: 0.7595\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 167s 4ms/step - loss: 0.5694 - accuracy: 0.8065 - val_loss: 0.7199 - val_accuracy: 0.7743\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 168s 4ms/step - loss: 0.5639 - accuracy: 0.8097 - val_loss: 0.7880 - val_accuracy: 0.7571\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - 171s 4ms/step - loss: 0.5441 - accuracy: 0.8158 - val_loss: 0.8622 - val_accuracy: 0.7560\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 162s 4ms/step - loss: 0.5427 - accuracy: 0.8186 - val_loss: 0.7022 - val_accuracy: 0.7711\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 172s 4ms/step - loss: 0.5426 - accuracy: 0.8183 - val_loss: 0.7457 - val_accuracy: 0.7828\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 171s 4ms/step - loss: 0.5443 - accuracy: 0.8213 - val_loss: 0.7111 - val_accuracy: 0.7797\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - 176s 4ms/step - loss: 0.5343 - accuracy: 0.8214 - val_loss: 0.8170 - val_accuracy: 0.7581\n",
      "10000/10000 [==============================] - 15s 2ms/step\n",
      "Test score: 0.8441471182823181\n",
      "Test accuracy: 0.7556999921798706\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import os\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context # Used to bypass SSL certificate verification\n",
    "os.makedirs('preview', exist_ok=True)# Make directory for augmentation of images\n",
    "NUM_TO_AUGMENT=5\n",
    "\n",
    "#CIFAR_10 is a set  of 60K images 32x32 pixels on Flatten 3 channels\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "#constant\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 20\n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = RMSprop()\n",
    "#load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "#augementing\n",
    "print(\"Augmenting training set images...\")\n",
    "# convert to categorical\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# float and normalization\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "# network\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "# train\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM, metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT, verbose=VERBOSE)\n",
    "score = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "#save model\n",
    "model_json = model.to_json()\n",
    "open('cifar10_architecture.json', 'w').write(model_json)\n",
    "#And the weights learned by our deep network on the training set\n",
    "model.save_weights('cifar10_weights.h5', overwrite=True)\n",
    "datagen = ImageDataGenerator( rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "xtas, ytas = [], []\n",
    "for i in range(X_train.shape[0]):\n",
    "    num_aug = 0\n",
    "    x = X_train[i] # (3, 32, 32)\n",
    "    x = x.reshape((1,) + x.shape) # (1, 3, 32, 32)\n",
    "for x_aug in datagen.flow(x, batch_size=1, save_to_dir='preview', save_prefix='cifar', save_format='jpeg'):\n",
    "    if num_aug >= NUM_TO_AUGMENT:\n",
    "        break\n",
    "    xtas.append(x_aug[0])\n",
    "num_aug += 1\n",
    "#fit the dataget\n",
    "datagen.fit(X_train)\n",
    "#train\n",
    "history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE), samples_per_epoch=X_train.shape[0], epochs=NB_EPOCH, verbose=VERBOSE)\n",
    "score = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm, applied to other data sets, poses significant ethical and privacy implications. When utilizing a data set comprised of people’s faces, sensitive information may be obtained in unauthorized ways, stripping public anonymity and individual safety. With public access to this tool, any individual could upload a picture of an individual they potentially have malicious intent towards and find sensitive information. This infringement of privacy and identification of an unknown individual is described as \"users would potentially be able to identify every person they saw. The tool could identify activists at a protest or an attractive stranger on the subway, revealing not just their names but where they lived, what they did, and whom they knew\" (Hill, 2020). This poses a significant privacy issue for an individual, as their private identifiable information can be easily retrieved by a third party without their knowledge. Subsequently, this identification can manifest in the weaponization of the tool, such as by \"a rogue law enforcement officer who wants to stalk potential romantic partners or a foreign government using this to dig up secrets about people to blackmail them or throw them in jail\" (Hill, 2020). Additionally, \"deep learning models appear to often memorize rare details about the training data that are completely unrelated to the intended task while the model is still learning the underlying behavior\" (Carlini, Liu, Erlingsson, Kos, & Song, 2019). This can raise additional privacy concerns as the algorithm may remember highly sensitive information, even if it only appears sporadically, which can later be extracted and maliciously utilized by a separate adversary.\n",
    "\n",
    "Next, this algorithm also poses ethical considerations if utilized in a self-driving vehicle. As each city has a different demographic makeup, sampling bias can be a significant threat to the training data used by the algorithm. An example of this demographic difference is described as \"a pedestrian recognition model trained on only pictures of pedestrians in rural America will not operate well in a multicultural urban city because pedestrians from the two populations would not have similar appearances\" (Xiang, 2019). Inadvertently, the algorithm may incorporate this bias when identifying pedestrians along the road. This manifestation of bias allows for higher precision based on skin tones, described as \"evidence that standard models for the task of object detection, trained on standard datasets, appear to exhibit higher precision on lower Fitzpatrick skin types than higher skin types. This behavior appears on large images of pedestrians and even grows when we remove occluded pedestrians\" (Wilson, Hoffman, & Morgenstern, 2019). Subsequently, self-driving vehicles may introduce accidents or fatalities due to their lower fidelity across different demographic groups.\n",
    "\n",
    "References\n",
    "\n",
    "Carlini, N., Liu, C., Erlingsson, Ú., Kos, J., & Song, D. (2019). The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks. In SEC’19: Proceedings of the 28th USENIX Conference on Security Symposium (pp. 267–284). USENIX Association.\n",
    "\n",
    "Hill, K. (2020, January 18). The Secretive Company That Might End Privacy as We Know It. The New York Times. https://www.nytimes.com/2020/01/18/technology/clearview-privacy-facial-recognition.html\n",
    "\n",
    "Wilson, B., Hoffman, J., & Morgenstern, J. (2019). Predictive Inequity in Object Detection. ArXiv, abs/1902.11097.\n",
    "\n",
    "Xiang, M. (2019, March 17). Human Bias in Machine Learning. Medium. https://towardsdatascience.com/bias-what-it-means-in-the-big-data-world-6e64893e92a1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
